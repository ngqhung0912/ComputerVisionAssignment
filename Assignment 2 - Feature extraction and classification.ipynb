{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609bcc26",
   "metadata": {},
   "source": [
    "# Assignment 2 - Feature extraction and classification\n",
    "\n",
    "Note: This notebook file for the assignment has deviations from the course guide with respect to the structure, sentence framing, question framing and numbering. Please consider this notebook file structure as the final structure and follow this.\n",
    "\n",
    "In this assignment, you are expected to\n",
    "\n",
    "(1) extract global features from CIFAR10 dataset with one of the pre-trained neural networks available in pytorch,\n",
    "\n",
    "(2) classify the dataset using the traditional k-Nearest Neighbours classifier,\n",
    "\n",
    "and\n",
    "\n",
    "(3) implement k-fold cross-validation to evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b394f7",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4d6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the needed packages for this assignment here\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838957a",
   "metadata": {},
   "source": [
    "When working with Pytorch, dataloader() is a must to know function. Read more about this function and the parameters it accepts in https://blog.paperspace.com/dataloaders-abstractions-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7db27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2320b0",
   "metadata": {},
   "source": [
    "The variable 'transform' encapsulates the needed transformations of our data. Read more about transforms in https://blog.paperspace.com/dataloaders-abstractions-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3daee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # resize\n",
    "    transforms.Resize(32),\n",
    "    # center-crop\n",
    "    transforms.CenterCrop(32),\n",
    "    # to-tensor\n",
    "    transforms.ToTensor(),\n",
    "    # normalize\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254f0e38",
   "metadata": {},
   "source": [
    "### INPUT DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ea732",
   "metadata": {},
   "source": [
    "Load the CIFAR10 dataset from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb189026",
   "metadata": {},
   "source": [
    "#### Exercise 2.1 - Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57aaaa8",
   "metadata": {},
   "source": [
    "**a)** Write a function **'train_test_split(dataset, ratio)'** which takes a dataset array as an input and returns two dataset arrays- one for training and another for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataset, ratio):\n",
    "    \n",
    "    # Ex. 2.1a your code here\n",
    "    \n",
    "    return training_data,testing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f160f",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION\n",
    "\n",
    "Extract descriptros from the images in your train and test dataset. The dataset split should remain the same for all the experiments if you want to be fair when comparing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032fccdb",
   "metadata": {},
   "source": [
    "#### Exercise 2.2 - Feature 1 - RGB descriptor\n",
    "\n",
    "Implement the same code you wrote for extracting the overall RGB descriptors(of size n x 24) as in assignment 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f654967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.2.2 your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51372d11",
   "metadata": {},
   "source": [
    "#### Exercise 2.3 - Feature 2 - Extract CNN descriptors using pre-traind networks\n",
    "\n",
    "Load one of the pretrained network (resnet, alexnet, vgg, squeezenet, densenet, inception) from pytorch to extract global features from the images present in the dataset. \n",
    "We will use the output values from the layer present just before the fully connected layer of the deep network as a descriptor, i.e. we will remove the last fully-connected layer. Therefore, after feed-forwarding the input image through the network, we save the output as the descriptor of the image. We do this for all the images present in the dataset to get the overall CNN descriptors.\n",
    "\n",
    "You may refer to this link for debugging purposes - https://stackoverflow.com/questions/52548174/how-to-remove-the-last-fc-layer-from-a-resnet-model-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e04373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Ex.2.3 your code here\n",
    "# name of the model you wish to use - it should be selected from this list\n",
    "# [resnet, alexnet, vgg, squeezenet, densenet, inception]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599bb2b",
   "metadata": {},
   "source": [
    "### PERFORMANCE EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a59fdf",
   "metadata": {},
   "source": [
    "#### Exercise 2.4 - Error function\n",
    "\n",
    "Implement a function to evaluate the accuracy of your prediction. We will rely on the evaluation metric 'accuracy'.\n",
    "\n",
    "You are suggested to also use f-score, recall and precision. Have a look at https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3677bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "    \n",
    "    # Ex.2.4 your code here\n",
    "    \n",
    "    return accuracy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b447784",
   "metadata": {},
   "source": [
    "### TRAIN AND TEST YOUR MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab251607",
   "metadata": {},
   "source": [
    "#### Exercise 2.5 - k Nearest Neighbour model\n",
    "\n",
    "For this exercise, first split the extracted overall RGB and CNN descriptor to train and test sets with the help of the 'train_test_split()' function that you implemented before.\n",
    "\n",
    "**a)** Apply the classifier with different values of k (number of nearest neighbours) to the train **RGB descriptor** set and evaluate the performance of your models using the accuracy_metric() function that you implemented before.\n",
    "\n",
    "You can have a look at the documentation to understand the parameters that define the learning of the model,\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1470832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Use your k-NN - play with the value of the parameters to see how the model performs\n",
    "kvalue_list = [2,4,6,10,15] \n",
    "\n",
    "# Ex.2.5a your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f26dee-1c7b-4498-a6aa-320a2106a5b9",
   "metadata": {},
   "source": [
    "**b)** Apply the classifier with different values of k (number of nearest neighbours) to the train **CNN descriptor** and evaluate the performance of your models using the accuracy_metric() function that you implemented before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90aba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.2.5b your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e94c23",
   "metadata": {},
   "source": [
    "#### Exercise 2.6 - Visualize results \n",
    "\n",
    "**a)** Since you already applied PCA to the extracted overall RGB descriptor in assignment 1, now apply PCA to the extracted overall **CNN descriptor**.\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "1) Choose the kNN classifier with k value that gave you the best results in the previous exercise and use it to make predictions on your train CNN descriptor set.\n",
    "\n",
    "2) Apply PCA on the train set and select the first 2 principal components to represent each sample.\n",
    "\n",
    "2) Plot the principal components representing the samples with empty circles. Use one color per ground truth class lables. On top of this, plot the samples again but now with filled circles. For these filled circles, use the color of the class predicted per sample in step 1. You can note that misclassifications will make the colours not coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.2.6a your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a156163-1ee6-42f9-a187-d0738caebf81",
   "metadata": {},
   "source": [
    "**b)** Repeat the steps mentioned before but now on the test CNN descriptor set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da86aaad-02cb-4919-826f-0bb254a29da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex.2.6b your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77cbfb5",
   "metadata": {},
   "source": [
    "#### Exercise 2.7 - kNN with k-Fold cross-validation\n",
    "\n",
    "Assess the performance of your implemented kNN using k-Fold cross-validation. \n",
    "\n",
    "Run your implemented function evaluating for k (fold) = 2, 5 and 10. You can rely on the kNN that performed best in the previous exercises.\n",
    "Report the average accuracy and the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f603d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Ex.2.7 your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba435dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SUGGESTION ON HOW TO PRESENT PERFORMANCE OF YOUR KFOLD CROSS VALIDATION ANALYSIS\n",
    "\n",
    "print('Summary results:')\n",
    "print(' ')\n",
    "print(' ')\n",
    "for i,k in enumerate(k_list):\n",
    "    print(k,'-fold cross validation:')  \n",
    "    print('Accuracies per fold: ', avg_acc_list[i]) \n",
    "    \n",
    "    avg_acc = round(sum(avg_acc_list[i])/k,2)\n",
    "    std_list= round(np.std(avg_acc_list[i]),2)\n",
    "    print('Average accuracy: ', avg_acc,'+-', std_list) \n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf70eb",
   "metadata": {},
   "source": [
    "### [Optional] Exercise: further explore by: \n",
    "- implement other classifiers such as SVM or Random Forest, \n",
    "- extract other descriptors from the images such as objects or other local features,\n",
    "- implement the evaluation metrics: recall, precision and f-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a83da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
